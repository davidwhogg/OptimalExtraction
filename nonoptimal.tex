\documentclass[12pt, letterpaper]{article}

\addtolength{\topmargin}{-0.75in}
\addtolength{\textheight}{1.50in}
\setlength{\parindent}{1.1\baselineskip}

\begin{document}\sloppy\sloppypar\raggedbottom\frenchspacing

\noindent
\textbf{Warning:}
This document is \textbf{deprecated}.
Most of the important ideas here are already in the Zechmeister \textsl{et al.}
paper on Flat-relative optimal extraction.

\section*{A think-o in spectroscopic optimal extraction}

\noindent
\textbf{David W. Hogg}\\
{\footnotesize
\textsl{Center for Cosmology and Particle Physics, New York University}\\
\textsl{Max-Planck-Institut f\"ur Astronomie}\\
\textsl{Flatiron Institute, a division of the Simons Foundation}
}

\paragraph{Abstract:}
The classic method of \emph{optimal extraction} for obtaining
one-dimensional spectra from two-dimensional spectrograph images
operates by constructing an ``optimal'' (in a certain sense) weighted
sum of sky-subtracted pixel values.
This optimal sum looks like a regression or a matched filter;
that's a good outcome, and correct.
The original paper, however, relies on two related, unnecessary, and
at least slightly incorrect, assumptions.
The first is that the data minus the sky divided by the point-spread
function will deliver an unbiased estimate of the true spectral
intensity.
The second is that the instrument PSF has a certain normalization
property, which requires that the data have been perfectly
flat-fielded, and that the associated noise estimates were adjusted
correctly in response.
Oddly, neither of these assumptions is required; both can be dropped.
The new expression for optimal extraction is very similar to the
original formula.
However it is simpler, less sensitive to uncertainties in the
flat-field, and correct under weaker assumptions.
If spectroscopy projects adopt the new expression presented here, it
will improve and simplify their data-analysis pipelines.
You're welcome.

\section{Optimal extraction: What's wrong with it?}

In the original paper (Horne, 1986), but modifying some indexing, the
optimal-extraction algorithm is given as
\begin{equation}
f_\lambda \leftarrow \frac{\sum_j W_{j\lambda}\,(D_{j\lambda} - S_{j\lambda}) / P_{j\lambda}}%
                          {\sum_j W_{j\lambda}}
\quad ,
\end{equation}
where
the index $j$ indexes pixels in the spatial direction,
the index $\lambda$ indexes pixels in the wavelength direction,
$f_\lambda$ is an estimate of the flux of the source at wavelength
(or really in the column or row) $\lambda$,
the $W_{j\lambda}$ are ``optimal'' weights for wavelength lambda,
which we will give in a moment,
the $D_{j\lambda}$ are the pixel intensities in pixels $j$,
the $S_{j\lambda}$ are sky intensities in pixels $j$
(presumed precisely and accurately known, hahaha), and
the $P_{j\lambda}$ represents the point-spread function,
about which we will say more below.
In this setting, the idea is that the spectrograph optics produce a
two-dimensional spectrum so ``orthogonal'' that we can perform
extraction at a single ``column'' (or row) of wavelength $\lambda$ by
just doing a sum over the pixels $j$ in that column.
I will say words about this a bit below.

The weights $W_{j\lambda}$ that minimize variance are given by
\begin{equation}
W_{j\lambda} = \frac{P_{j\lambda}^2}{V_{j\lambda}}
\quad ,
\end{equation}
where $V_{j\lambda}$ is the variance of the noise in pixel $j$ (which,
presumably, is supposed to be the sum of the noise in the data and the
sky estimate.
And, by assumption, the point-spread function obeys a normalization
condition
\begin{equation}
1 = \sum P_{j\lambda}
\quad ,
\end{equation}
which makes the definition of $P_{j\lambda}$ the probability that a
\emph{detected} photon in column (or row) $\lambda$ was detected in
row (or column) $j$.
Note that this is to be distinguished from the probability of a photon
of wavelength $\lambda$ at the top of the atmosphere being detected in
pixel $j$.
Or from the probability of a photon at the top of the atmosphere that
would (if detected) end up in column (or row) $\lambda$ being (in
fact) detected in row (or column) $j$.
That is, $P_{j\lambda}$ is effectively the PSF in perfectly
flat-fielded data.
Thus the optimal extraction paper assumes not just that the data are
zero-, dark-, and bias- subtracted, but also that they have been
flat-fielded to a precision that is so good that nothing could
possibly go wrong.

The algorithm is justified by saying that the ratios
$(D_{j\lambda}-S_{j\lambda})/P_{j\lambda}$ are unbiased estimates of
$f_\lambda$.
\emph{If} this is the case, and if all the pixels are independent, and
\emph{if} all the uncertainty variance estimates $V_{j\lambda}$ are
sufficiently accurate, then it is true that this setting of the
weights$W_{j\lambda}$ will indeed deliver the minimum-variance
weighted mean of those unbiased estimates.

Unfortunately, this is not true for several reasons, all of which
relate to the fact that you can't know your point-spread function or
your flat-field accurately in locations in your spectrograph that are
not well illuminated by your object or your calibration lamps.

For example... explain what an EPRV spectrograph looks like these days
HOGG.

When you don't know this precisely, either the flat-fielding goes
wrong, or else the division by the point-spread function delivers
biased estimates of the intensity, or the correction of the data
uncertainties spoils the variance minimization, or all of the above.

What I will show (below) is that if you do everything consistently
between the flat-fielding and the data-uncertainty estimation, the
Horne expressions are not incorrect, they are just numerically
unstable.

But there is no need to introduce so opportunities for inconsistencies
or numerical instability when they aren't needed.

A more general statement is that you don't want to be correcting your
data for the flat-field; you want to be correcting your model for the
flat-field.
And in particular, you don't want to be dividing your data by tiny
numbers!

\section{The new optimal extraction}

If I got to decide, I would call the (zero-, dark-, and
bias-subtracted but \emph{not} flat-fielded) image data $y$ (which is
a two-dimensional image), a column (or row) of the data $y_{\lambda}$
(which is a vector or list), and a pixel $y_{j\lambda}$ (which is a
scalar).
I would call the point-spread function (PSF) at column (or row)
$\lambda$ $\psi_\lambda$.
This is also a vector or list..

But importantly I would not normalize this PSF to unity. I would DO
SOMETHING WHAT HOGG?

AND DESCRIBE THE SKY NOW.

Our belief is that the vector $y_{\lambda}$ (the set of pixels
$y_{j\lambda}$ for all $j$) is generated by a true flux $f_{\lambda}$
times the PSF, plus sky:
\begin{equation}
E[y_\lambda] = s_\lambda + f_\lambda\,\psi_\lambda
\end{equation}
That is, the expectation for the data in column (or row) $\lambda$ is
a sum of the sky model and the source model.

I would presume that the 

I would change the notation to the following ...

In my notation, the Horne optimal extraction algorithm becomes ...

There are proofs in statistics that the minimum-variance unbiased
estimator of a quantity is also the maximum-likelihood value for that
quantity, given the data.

...

\section{Extensions and discussion}

The point about covariant noise.

The point that they sky and the source should be modeled
simultaneously, not subtracted.

The point that the sky, the source, and the data used to determine the
PSF should all be modeled simultaneously!

And the flat, dark, and so on. Why not? If precision requirements are
high!

In addition, the optimal extraction paper makes VERY STRONG
ASSUMPTIONS about how the spectrum is laid out on the
detector.... HOGG

\section*{References}
\begin{list}{}{%
\rightmargin=0in
\leftmargin=\parindent
\itemindent=-1.0\leftmargin
\listparindent=0.0\leftmargin
}
\item Horne, K., 1986,
An optimal extraction algorithm for CCD spectroscopy,
PASP 98 609.
\end{list}

\end{document}
